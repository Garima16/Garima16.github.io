---
layout : post
title : Recurrent Neural Networks
---

Recurrent Neural Networks or RNNs is a kind of sequence model(others include LSTM, GRU etc.) which were specifically designed for processing sequential data.
Before diving deep into the recurring world of RNNs, let's first understand what Sequence Models are and why do they perform better than traditional Feed Forward Neural Networks when dealing with sequential data. 

## Sequence Modeling
Sequence models are useful for processing sequential data like text(text is a sequence of sentences), time-series data, music, audio etc. Sequence models help to remember context in data being processed, like in natural language understanding(NLU) tasks, itâ€™s important to keep context in mind to fully understand the meaning of text.
 
 For example, consider this sentence, "Mark was reading a blog on sequence models and he got fascinated by them". If we want a model to fully comprehend the meaning of this sentence, then, it should not be difficult for model to make out that "_he_" refers to "_Mark_", and "_them_" refers to "_sequence models_". The model should remember what is being talked about and not get confused when such pronouns are encountered.

Traditional neural networks cannot persist context and treats every input as independent of others. When dealing with languages, we need to retain some past information, that we have encountered so far. Thus, to fully understand the meaning of any sentence, we require contextual knowledge, which, traditional neural networks cannot provide. Sequence models overcome this shortcoming by introducing a recurrent layer which allows information to persist. Recurrent layers take input at each timestep, and produce output by multiplying the input with a set of weights which is shared across all timesteps.

## What are RNNs?
RNNs can approximate any function involving recurrence. They represent arbitrary sized sequential inputs as fixed-size vectors(often called as context vector), which are often lossy. Depending upon how the network is trained, this lossy summary output might keep some aspects of the input data more precisely than some other aspects.
 
 RNN architecture shown below(inspired from figure 10.5 in [Goodfellow et al.,2016](https://d1wqtxts1xzle7.cloudfront.net/55326164/deeplearningbook.pdf?1513657574=&response-content-disposition=inline%3B+filename%3DDeep_Learning.pdf&Expires=1596485903&Signature=UonXVdG8GmnxtInq6YgjgIx9veQI1O7dbfHUXRcHhvS1MtLaELZvA9IfdibSSd7fkwYl-C0oxI7-bacU7H9LQ9Hr5KYasXhRcH5Q8Ngnj2j4w9zkgCs7U83JHFqCB5k3YWUzkmJJzxOwvYhHyrXyOCBkkHjRY7i0STYNV~j5rzEfbXjAqaHHApin-ifDNb-7h3wq5eTGN8tW4ezQq~drgSJ5p2ASXaxBpmGsLTIFa5w22RZeHjAb416IWgM~ooQtjZF3~Kkz~k93cV-MKZLbdV6HYabXvgnu5NIx5LnkbEWacq5IQm3Fok0VlzeSXrcWofkNK3SNxT9VWed1gAZayQ__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA)) takes as input a temporal sequence of size T . The network is designed for tasks like sentiment classification(or any other classification task), with a single output predicted at the end of the reading the entire sequence. At the end, loss is computed, which, in this case is categorical cross entropy.

![rnn architecture](/assets/images/rnn.png "RNN Architecture")

## Design Patterns in RNNs 

This section is inspired from [Goodfellow et al.,2016](https://d1wqtxts1xzle7.cloudfront.net/55326164/deeplearningbook.pdf?1513657574=&response-content-disposition=inline%3B+filename%3DDeep_Learning.pdf&Expires=1596485903&Signature=UonXVdG8GmnxtInq6YgjgIx9veQI1O7dbfHUXRcHhvS1MtLaELZvA9IfdibSSd7fkwYl-C0oxI7-bacU7H9LQ9Hr5KYasXhRcH5Q8Ngnj2j4w9zkgCs7U83JHFqCB5k3YWUzkmJJzxOwvYhHyrXyOCBkkHjRY7i0STYNV~j5rzEfbXjAqaHHApin-ifDNb-7h3wq5eTGN8tW4ezQq~drgSJ5p2ASXaxBpmGsLTIFa5w22RZeHjAb416IWgM~ooQtjZF3~Kkz~k93cV-MKZLbdV6HYabXvgnu5NIx5LnkbEWacq5IQm3Fok0VlzeSXrcWofkNK3SNxT9VWed1gAZayQ__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA)

1. RNNs that produce an output at each time step and have recurrent connections between hidden units
![RNN design pattern 1](/assets/images/rnn1.png "RNN design pattern 1")

2. RNNs that produce an output at each time step and have recurrent connections only from output at one time step
to hidden units at next time step.
![RNN design pattern 2](/assets/images/rnn2.png "RNN design pattern 2")

3. RNNs with recurrent connections between hidden units, that read an entire sequence and then produce single output.
![RNN design pattern 3](/assets/images/rnn3.png "RNN design pattern 3")
